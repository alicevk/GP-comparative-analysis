{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AdDaNPjoYuHk",
        "outputId": "ad1d1195-288e-4309-da4f-362356c4331f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-04 12:28:41,225] A new study created in RDB with name: gp_scikit_heart_1\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py:477: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  _check_optimize_result(\"lbfgs\", opt_res)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py:477: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  _check_optimize_result(\"lbfgs\", opt_res)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py:477: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  _check_optimize_result(\"lbfgs\", opt_res)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "[I 2024-09-04 12:31:27,500] Trial 0 finished with value: 0.889918414918415 and parameters: {'kernel': 'matern', 'n_restarts_optimizer': 8, 'max_iter_predict': 531}. Best is trial 0 with value: 0.889918414918415.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:419: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "[I 2024-09-04 12:31:53,989] Trial 1 finished with value: 0.5 and parameters: {'kernel': 'white', 'n_restarts_optimizer': 5, 'max_iter_predict': 561}. Best is trial 0 with value: 0.889918414918415.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py:477: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  _check_optimize_result(\"lbfgs\", opt_res)\n",
            "[W 2024-09-04 12:32:45,080] Trial 2 failed with parameters: {'kernel': 'dot', 'n_restarts_optimizer': 10, 'max_iter_predict': 134} because of the following error: ValueError('\\nAll the 10 fits failed.\\nIt is very likely that your model is misconfigured.\\nYou can try to debug the error by setting error_score=\\'raise\\'.\\n\\nBelow are more details about the failures:\\n--------------------------------------------------------------------------------\\n2 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\\n    return fit_method(estimator, *args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\\n    self.base_estimator_.fit(X, y)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\\n    self._constrained_optimization(obj_func, theta_initial, bounds)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\\n    opt_res = scipy.optimize.minimize(\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\\n    sf = ScalarFunction(fun, x0, args, grad, hess,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\\n    self._update_fun()\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\\n    self._update_fun_impl()\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\\n    self.f = fun_wrapped(self.x)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\\n    fx = fun(np.copy(x), *args)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\\n    self._compute_if_needed(x, *args)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\\n    fg = self.fun(x, *args)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\\n    lml, grad = self.log_marginal_likelihood(\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\\n    L = cholesky(B, lower=True)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\\n    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\\nnumpy.linalg.LinAlgError: 118-th leading minor of the array is not positive definite\\n\\n--------------------------------------------------------------------------------\\n7 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\\n    return fit_method(estimator, *args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\\n    self.base_estimator_.fit(X, y)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\\n    self._constrained_optimization(obj_func, theta_initial, bounds)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\\n    opt_res = scipy.optimize.minimize(\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\\n    sf = ScalarFunction(fun, x0, args, grad, hess,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\\n    self._update_fun()\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\\n    self._update_fun_impl()\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\\n    self.f = fun_wrapped(self.x)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\\n    fx = fun(np.copy(x), *args)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\\n    self._compute_if_needed(x, *args)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\\n    fg = self.fun(x, *args)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\\n    lml, grad = self.log_marginal_likelihood(\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\\n    L = cholesky(B, lower=True)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\\n    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\\nnumpy.linalg.LinAlgError: 115-th leading minor of the array is not positive definite\\n\\n--------------------------------------------------------------------------------\\n1 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\\n    return fit_method(estimator, *args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\\n    self.base_estimator_.fit(X, y)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\\n    self._constrained_optimization(obj_func, theta_initial, bounds)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\\n    opt_res = scipy.optimize.minimize(\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\\n    sf = ScalarFunction(fun, x0, args, grad, hess,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\\n    self._update_fun()\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\\n    self._update_fun_impl()\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\\n    self.f = fun_wrapped(self.x)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\\n    fx = fun(np.copy(x), *args)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\\n    self._compute_if_needed(x, *args)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\\n    fg = self.fun(x, *args)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\\n    lml, grad = self.log_marginal_likelihood(\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\\n    L = cholesky(B, lower=True)\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\\n    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\\nnumpy.linalg.LinAlgError: 117-th leading minor of the array is not positive definite\\n').\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-2-cb3d9f8a87ae>\", line 99, in <lambda>\n",
            "    study.optimize(lambda trial: objective_function(trial, train_X, train_y), n_trials=Num_trials)\n",
            "  File \"<ipython-input-2-cb3d9f8a87ae>\", line 87, in objective_function\n",
            "    metrics = cross_val_score(model, X, y, scoring=Score, cv=Num_folds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 562, in cross_val_score\n",
            "    cv_results = cross_validate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 214, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 328, in cross_validate\n",
            "    _warn_or_raise_about_fit_failures(results, error_score)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 414, in _warn_or_raise_about_fit_failures\n",
            "    raise ValueError(all_fits_failed_message)\n",
            "ValueError: \n",
            "All the 10 fits failed.\n",
            "It is very likely that your model is misconfigured.\n",
            "You can try to debug the error by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "2 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\n",
            "    self.base_estimator_.fit(X, y)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\n",
            "    self._constrained_optimization(obj_func, theta_initial, bounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\n",
            "    opt_res = scipy.optimize.minimize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\n",
            "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\n",
            "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\n",
            "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\n",
            "    self._update_fun()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\n",
            "    self._update_fun_impl()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\n",
            "    self.f = fun_wrapped(self.x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\n",
            "    fx = fun(np.copy(x), *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\n",
            "    self._compute_if_needed(x, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\n",
            "    fg = self.fun(x, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\n",
            "    lml, grad = self.log_marginal_likelihood(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\n",
            "    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\n",
            "    L = cholesky(B, lower=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\n",
            "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\n",
            "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
            "numpy.linalg.LinAlgError: 118-th leading minor of the array is not positive definite\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "7 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\n",
            "    self.base_estimator_.fit(X, y)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\n",
            "    self._constrained_optimization(obj_func, theta_initial, bounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\n",
            "    opt_res = scipy.optimize.minimize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\n",
            "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\n",
            "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\n",
            "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\n",
            "    self._update_fun()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\n",
            "    self._update_fun_impl()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\n",
            "    self.f = fun_wrapped(self.x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\n",
            "    fx = fun(np.copy(x), *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\n",
            "    self._compute_if_needed(x, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\n",
            "    fg = self.fun(x, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\n",
            "    lml, grad = self.log_marginal_likelihood(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\n",
            "    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\n",
            "    L = cholesky(B, lower=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\n",
            "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\n",
            "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
            "numpy.linalg.LinAlgError: 115-th leading minor of the array is not positive definite\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "1 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\n",
            "    self.base_estimator_.fit(X, y)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\n",
            "    self._constrained_optimization(obj_func, theta_initial, bounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\n",
            "    opt_res = scipy.optimize.minimize(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\n",
            "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\n",
            "    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\n",
            "    sf = ScalarFunction(fun, x0, args, grad, hess,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\n",
            "    self._update_fun()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\n",
            "    self._update_fun_impl()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\n",
            "    self.f = fun_wrapped(self.x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\n",
            "    fx = fun(np.copy(x), *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\n",
            "    self._compute_if_needed(x, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\n",
            "    fg = self.fun(x, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\n",
            "    lml, grad = self.log_marginal_likelihood(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\n",
            "    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\n",
            "    L = cholesky(B, lower=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\n",
            "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\n",
            "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
            "numpy.linalg.LinAlgError: 117-th leading minor of the array is not positive definite\n",
            "\n",
            "[W 2024-09-04 12:32:45,083] Trial 2 failed with value None.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\n    self.base_estimator_.fit(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\n    self._constrained_optimization(obj_func, theta_initial, bounds)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\n    opt_res = scipy.optimize.minimize(\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\n    self._update_fun()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\n    self._update_fun_impl()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\n    self.f = fun_wrapped(self.x)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\n    fx = fun(np.copy(x), *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\n    self._compute_if_needed(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\n    fg = self.fun(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\n    lml, grad = self.log_marginal_likelihood(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\n    L = cholesky(B, lower=True)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\n    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\nnumpy.linalg.LinAlgError: 118-th leading minor of the array is not positive definite\n\n--------------------------------------------------------------------------------\n7 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\n    self.base_estimator_.fit(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\n    self._constrained_optimization(obj_func, theta_initial, bounds)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\n    opt_res = scipy.optimize.minimize(\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\n    self._update_fun()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\n    self._update_fun_impl()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\n    self.f = fun_wrapped(self.x)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\n    fx = fun(np.copy(x), *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\n    self._compute_if_needed(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\n    fg = self.fun(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\n    lml, grad = self.log_marginal_likelihood(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\n    L = cholesky(B, lower=True)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\n    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\nnumpy.linalg.LinAlgError: 115-th leading minor of the array is not positive definite\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\n    self.base_estimator_.fit(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\n    self._constrained_optimization(obj_func, theta_initial, bounds)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\n    opt_res = scipy.optimize.minimize(\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\n    self._update_fun()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\n    self._update_fun_impl()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\n    self.f = fun_wrapped(self.x)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\n    fx = fun(np.copy(x), *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\n    self._compute_if_needed(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\n    fg = self.fun(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\n    lml, grad = self.log_marginal_likelihood(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\n    L = cholesky(B, lower=True)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\n    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\nnumpy.linalg.LinAlgError: 117-th leading minor of the array is not positive definite\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cb3d9f8a87ae>\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m#study.optimize(lambda trial: objective_function(trial, train_X_np, train_y_np), n_trials=Num_trials)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNum_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;31m# Save and display the best results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mtrialdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-cb3d9f8a87ae>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m#study.optimize(lambda trial: objective_function(trial, train_X_np, train_y_np), n_trials=Num_trials)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNum_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;31m# Save and display the best results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mtrialdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-cb3d9f8a87ae>\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(trial, X, y, Num_folds, random_state)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_instance_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mScore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNum_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# For callable scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             )\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\n    self.base_estimator_.fit(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\n    self._constrained_optimization(obj_func, theta_initial, bounds)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\n    opt_res = scipy.optimize.minimize(\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\n    self._update_fun()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\n    self._update_fun_impl()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\n    self.f = fun_wrapped(self.x)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\n    fx = fun(np.copy(x), *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\n    self._compute_if_needed(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\n    fg = self.fun(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\n    lml, grad = self.log_marginal_likelihood(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\n    L = cholesky(B, lower=True)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\n    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\nnumpy.linalg.LinAlgError: 118-th leading minor of the array is not positive definite\n\n--------------------------------------------------------------------------------\n7 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\n    self.base_estimator_.fit(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\n    self._constrained_optimization(obj_func, theta_initial, bounds)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\n    opt_res = scipy.optimize.minimize(\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\n    self._update_fun()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\n    self._update_fun_impl()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\n    self.f = fun_wrapped(self.x)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\n    fx = fun(np.copy(x), *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\n    self._compute_if_needed(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\n    fg = self.fun(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\n    lml, grad = self.log_marginal_likelihood(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\n    L = cholesky(B, lower=True)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\n    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\nnumpy.linalg.LinAlgError: 115-th leading minor of the array is not positive definite\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 741, in fit\n    self.base_estimator_.fit(X, y)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 246, in fit\n    self._constrained_optimization(obj_func, theta_initial, bounds)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 474, in _constrained_optimization\n    opt_res = scipy.optimize.minimize(\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 713, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\", line 347, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 288, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 166, in __init__\n    self._update_fun()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 262, in _update_fun\n    self._update_fun_impl()\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 163, in update_fun\n    self.f = fun_wrapped(self.x)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\", line 145, in fun_wrapped\n    fx = fun(np.copy(x), *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 79, in __call__\n    self._compute_if_needed(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\", line 73, in _compute_if_needed\n    fg = self.fun(x, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 220, in obj_func\n    lml, grad = self.log_marginal_likelihood(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 385, in log_marginal_likelihood\n    Z, (pi, W_sr, L, b, a) = self._posterior_mode(K, return_temporaries=True)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/_gpc.py\", line 444, in _posterior_mode\n    L = cholesky(B, lower=True)\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 88, in cholesky\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\", line 36, in _cholesky\n    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\nnumpy.linalg.LinAlgError: 117-th leading minor of the array is not positive definite\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF, DotProduct, Matern, RationalQuadratic, WhiteKernel\n",
        "\n",
        "from optuna import create_study, Trial\n",
        "\n",
        "# General parameters\n",
        "Test_Size = 0.2\n",
        "Random_Seed = 82024\n",
        "Num_trials = 1000\n",
        "Num_folds = 10\n",
        "Study_name = \"gp_scikit_heart_1\"\n",
        "Score = \"roc_auc\"  # Or \"f1\"\n",
        "\n",
        "# Kernel setup\n",
        "Kernels = {\n",
        "    \"rbf\": 1 * RBF(),\n",
        "    \"dot\": 1 * DotProduct(),\n",
        "    \"matern\": 1 * Matern(),\n",
        "    \"quad\": 1 * RationalQuadratic(),\n",
        "    \"white\": 1 * WhiteKernel(),\n",
        "}\n",
        "\n",
        "# Data handling\n",
        "# Fetch dataset from UCI Repository\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "heart_disease = fetch_ucirepo(id=45)\n",
        "df = heart_disease.data.original\n",
        "\n",
        "# Drop missing values\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Binarize data\n",
        "df.loc[df[\"num\"] != 0, \"num\"] = 1\n",
        "\n",
        "# Define features and target\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df['num'].values\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_index, test_index = train_test_split(list(range(y.size)), test_size=Test_Size, random_state=Random_Seed)\n",
        "\n",
        "train_df = df.loc[train_index]\n",
        "test_df = df.loc[test_index]\n",
        "\n",
        "# Convert to PyTorch tensors (double precision for compatibility with sklearn)\n",
        "train_X = torch.tensor(train_df.iloc[:, :-1].values).double()\n",
        "train_y = torch.tensor(train_df['num'].values).double()\n",
        "\n",
        "test_X = torch.tensor(test_df.iloc[:, :-1].values).double()\n",
        "test_y = torch.tensor(test_df['num'].values).double()\n",
        "\n",
        "# Convert back to NumPy arrays for scikit-learn\n",
        "#train_X_np = train_X.numpy()\n",
        "#train_y_np = train_y.numpy()\n",
        "\n",
        "#test_X_np = test_X.numpy()\n",
        "#test_y_np = test_y.numpy()\n",
        "\n",
        "# Function to create model instances\n",
        "def create_instance_model(trial):\n",
        "    \"\"\"Create an instance of the model.\"\"\"\n",
        "    kernel_id = trial.suggest_categorical(\"kernel\", [\"rbf\", \"white\", \"dot\", \"matern\", \"quad\"])\n",
        "\n",
        "    parameters = {\n",
        "        \"kernel\": Kernels[kernel_id],\n",
        "        \"n_restarts_optimizer\": trial.suggest_int(\"n_restarts_optimizer\", 0, 10),\n",
        "        \"max_iter_predict\": trial.suggest_int(\"max_iter_predict\", 50, 1000, log=True),\n",
        "        \"random_state\": Random_Seed,\n",
        "    }\n",
        "\n",
        "    model = GaussianProcessClassifier(**parameters)\n",
        "    return model\n",
        "\n",
        "# Objective function for Optuna\n",
        "def objective_function(trial, X, y, Num_folds=Num_folds, random_state=Random_Seed):\n",
        "    \"\"\"Optuna's objective function\"\"\"\n",
        "    model = create_instance_model(trial)\n",
        "\n",
        "    metrics = cross_val_score(model, X, y, scoring=Score, cv=Num_folds)\n",
        "    return metrics.mean()\n",
        "\n",
        "# Create the study with Optuna\n",
        "study = create_study(\n",
        "    study_name=Study_name,\n",
        "    storage=f\"sqlite:///{Study_name}.db\",\n",
        "    direction=\"maximize\",\n",
        "    load_if_exists=True,\n",
        ")\n",
        "\n",
        "#study.optimize(lambda trial: objective_function(trial, train_X_np, train_y_np), n_trials=Num_trials)\n",
        "study.optimize(lambda trial: objective_function(trial, train_X, train_y), n_trials=Num_trials)\n",
        "# Save and display the best results\n",
        "trialdf = study.trials_dataframe()\n",
        "trialdf.to_csv(\"trial_df.csv\", index=False)\n",
        "\n",
        "best_trial = study.best_trial\n",
        "print(best_trial)\n",
        "\n",
        "# Train and evaluate the final model\n",
        "model = create_instance_model(best_trial)\n",
        "#model.fit(train_X_np, train_y_np)\n",
        "model.fit(train_X, train_y)\n",
        "\n",
        "# Test the model\n",
        "#y_pred = model.predict(test_X_np)\n",
        "#pred_probs = model.predict_proba(test_X_np)\n",
        "y_pred = model.predict(test_X)\n",
        "pred_probs = model.predict_proba(test_X)\n",
        "\n",
        "# Model evaluation\n",
        "#acc = accuracy_score(test_y_np, y_pred)\n",
        "#roc_auc = roc_auc_score(test_y_np, pred_probs[:, 1])\n",
        "acc = accuracy_score(test_y, y_pred)\n",
        "roc_auc = roc_auc_score(test_y, pred_probs[:, 1])\n",
        "\n",
        "print(f\"Accuracy: {acc:.2f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsdR_QSAZF5Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
